{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from io import open\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "春 に なれ ば 花 が 咲き 、 夏 に は ほととぎす が 鳴き 、 秋 に は 月 が 美しく 、 冬 に は 雪 が 降っ て 身 が ひきしまる \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MeCab is an open-source text segmentation library for use with text written in the Japanese.\n",
    "import MeCab # pip install mecab-python3\n",
    "wakati = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "example = \"春になれば花が咲き、夏にはほととぎすが鳴き、秋には月が美しく、冬には雪が降って身がひきしまる。\".strip('。')\n",
    "print(wakati.parse(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN_SENTENCE = 10\n",
    "SOS = 0\n",
    "EOS = 1\n",
    "\n",
    "class Language:\n",
    "    def __init__(self, name_lang):\n",
    "        self.name = name_lang\n",
    "        self.word2index = {}\n",
    "        self.index2word = {SOS: \"SOS\", EOS: \"EOS\"}\n",
    "        self.vocab_size = 2\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.vocab_size\n",
    "        \n",
    "    def addWords(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            if word not in self.word2index:\n",
    "                self.index2word[self.vocab_size] = word\n",
    "                self.word2index[word] = self.vocab_size\n",
    "                self.vocab_size += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LanguageTranslate():\n",
    "    def __init__(self, input_lang, target_lang, file_path='./dictionary', limit_size=None):\n",
    "        self.input_lang = input_lang\n",
    "        self.target_lang = target_lang\n",
    "        self.pair_sentences = []\n",
    "        \n",
    "        file_name = \"{}-{}.txt\".format(self.input_lang.name, self.target_lang.name)\n",
    "        file_name = os.path.join(file_path, file_name)\n",
    "        lines = open(file_name).read().strip().split('\\n')\n",
    "        \n",
    "        self.pair_sentences = [[s for s in line.split('\\t')[:2]] for line in lines]\n",
    "        \n",
    "        # if the language is japanese do text segmentation\n",
    "        for s in self.pair_sentences:\n",
    "            if self.target_lang.name == 'jpn':\n",
    "                s[1] = wakati.parse(s[1]).strip(' \\n')#.strip('\\n')#.strip('。')\n",
    "                s[1] = s[1].strip('。')\n",
    "            else : s[1] = self._normalize(s[1])\n",
    "            s[0] = self._normalize(s[0])\n",
    "        \n",
    "        for pairs in self.pair_sentences: \n",
    "            self.input_lang.addWords(pairs[0])\n",
    "            self.target_lang.addWords(pairs[1])\n",
    "        \n",
    "        print('Pairs size (before filter):', len(self.pair_sentences))\n",
    "        self.pair_sentences = self._filterpairs(self.pair_sentences, lang_1=self.input_lang.name) \n",
    "        print('Pairs size (after filter):', len(self.pair_sentences))\n",
    "       \n",
    "    def _filterpairs(self, pairs, lang_1='eng'):\n",
    "        filter_prefixes = (\"the\", \"this\", \"that\", \"these\", \"those\") # must be lower case\n",
    "        pairs_filtered = []\n",
    "        # the filter is built over the first pair of\n",
    "        for pair in pairs:\n",
    "            if (len(pair[0].split(' ')) < MAX_LEN_SENTENCE) and (len(pair[1].split(' ')) < MAX_LEN_SENTENCE):\n",
    "                if (lang_1=='eng' and pair[0].startswith(filter_prefixes)):\n",
    "                    pairs_filtered.append(pair)\n",
    "        return pairs_filtered\n",
    "                    \n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pair_sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pair_sentences[idx]\n",
    "        \n",
    "        inp_lang = [self.input_lang.word2index[i] for i in pair[0].split(' ')]\n",
    "        inp_lang.append(EOS)\n",
    "        targ_lang = [self.target_lang.word2index[i] for i in pair[1].split(' ')]\n",
    "        targ_lang.append(EOS)\n",
    "        \n",
    "        inp_lang = torch.tensor(inp_lang, dtype=torch.long).view(-1, 1)\n",
    "        targ_lang = torch.tensor(targ_lang, dtype=torch.long).view(-1, 1)\n",
    "       \n",
    "        return inp_lang, targ_lang\n",
    "        \n",
    " \n",
    "    def _normalize(self, s):\n",
    "        s = s.lower() # everything to lower case\n",
    "        s = s.translate(str.maketrans('', '', string.punctuation)) # remove all punctuations\n",
    "        return s\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairs size (before filter): 45815\n",
      "pairs size (after filter): 2304\n",
      "Input language vocabulary size: 9390\n",
      "Target language vocabulary size: 13935\n"
     ]
    }
   ],
   "source": [
    "data_path = './dictionary'\n",
    "input_language = Language('eng')\n",
    "target_language = Language('jpn')\n",
    "\n",
    "dataset = LanguageTranslate(input_language, target_language, data_path)\n",
    "\n",
    "input_lang_vocab_size = dataset.input_lang.vocab_size\n",
    "target_lang_vocab_size = dataset.target_lang.vocab_size\n",
    "print(\"Input language vocabulary size:\", input_lang_vocab_size)\n",
    "print(\"Target language vocabulary size:\", target_lang_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pairs of sentences: 2304\n"
     ]
    }
   ],
   "source": [
    "train_size = len(dataset)\n",
    "batch_size = 1\n",
    "train_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "print('Number of pairs of sentences:', train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, embedding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.rnn = torch.nn.GRU(embedding_dim, hidden_size, num_layers=1)\n",
    "        \n",
    "    def forward(self, inputs, last_hidden):\n",
    "        embedded = self.embedding(inputs).view(1, 1, -1)\n",
    "        output, hidden = self.rnn(embedded, last_hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderAttn(nn.Module):\n",
    "    def __init__(self, hidden_size, output_vocab_size, embedding_dim, dropout_p=0.1, max_length=MAX_LEN_SENTENCE):\n",
    "        super(DecoderAttn, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(output_vocab_size, embedding_dim)\n",
    "        self.attn = nn.Linear(hidden_size + embedding_dim, max_length)\n",
    "        self.attn_combine = nn.Linear(hidden_size + embedding_dim, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_vocab_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = torch.cat((embedded[0], hidden[0]), 1)\n",
    "        attn_weights = self.attn(attn_weights)\n",
    "        attn_weights = F.softmax(attn_weights, dim=1)\n",
    "        \n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gpu: True\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Using gpu:\", use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hidden NN size and embedding dimensions definition\n",
    "embedding_dim = 256\n",
    "hidden_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(input_lang_vocab_size, hidden_size, embedding_dim).to(device)\n",
    "decoder = DecoderAttn(hidden_size, target_lang_vocab_size, embedding_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "encoder_optim = torch.optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optim = torch.optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "criterion = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(encoder_model, decoder_model, encoder_optim, decoder_optim, criterion, epochs=10, num_samples=75000, plots_per_samples=5000):\n",
    "    \n",
    "    cost = []\n",
    "    start_time = time.time()\n",
    "    loss = 0\n",
    "    \n",
    "    sample_acc_avg = 0\n",
    "    sample_loss_avg = 0\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        for inp_sentence, targ_sentence in train_loader:\n",
    "            sample_acc = 0\n",
    "            sample_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            loss = 0\n",
    "            inp_sentence = inp_sentence[0].to(device)\n",
    "            targ_sentence = targ_sentence[0].to(device)\n",
    "            \n",
    "            encoder_model.zero_grad()\n",
    "            decoder_model.zero_grad()\n",
    "\n",
    "            encoder_hidden = encoder_model.init_hidden()\n",
    "\n",
    "            all_encoder_outputs= torch.zeros(MAX_LEN_SENTENCE, encoder.hidden_size, device=device)\n",
    "            # run input sentence though encoder model\n",
    "            for i in range(inp_sentence.size(0)):\n",
    "                encoder_output, encoder_hidden = encoder_model(inp_sentence[i], encoder_hidden)\n",
    "                all_encoder_outputs[i] = encoder_output[0, 0] # get output in one dimension\n",
    "                \n",
    "\n",
    "            decoder_input = torch.tensor([[SOS]], dtype=torch.long).to(device)\n",
    "            decoder_hidden = encoder_hidden # the decoder use the same hidden layer used by the encoder\n",
    "\n",
    "            # run target sentence though decoder model\n",
    "            for i in range(targ_sentence.size(0)):\n",
    "                decoder_output, decoder_hidden, _ = decoder_model(decoder_input, decoder_hidden, all_encoder_outputs)\n",
    "\n",
    "                _, guess = decoder_output.topk(1)\n",
    "                decoder_input = guess.squeeze().detach()  # detach from history as input\n",
    "\n",
    "                loss += criterion(decoder_output, targ_sentence[i])\n",
    "                guess = guess[0].item()\n",
    "                if guess == int(targ_sentence[i]):\n",
    "                    running_corrects += 1\n",
    "\n",
    "                if decoder_input.item() == EOS:\n",
    "                    break\n",
    "\n",
    "            loss.backward() # backpropagation through time\n",
    "\n",
    "            encoder_optim.step() # update gradients of encoder\n",
    "            decoder_optim.step() # update gradients of decoder\n",
    "\n",
    "            sample_acc = running_corrects / targ_sentence.size(0)\n",
    "            sample_loss = loss.item() / targ_sentence.size(0)\n",
    "\n",
    "            cost.append(sample_loss)\n",
    "\n",
    "            sample_acc_avg += sample_acc\n",
    "            sample_loss_avg += sample_loss\n",
    "\n",
    "        sample_acc_avg /= train_size\n",
    "        sample_loss_avg /= train_size\n",
    "\n",
    "        time_elapsed = time.time() - start_time\n",
    "        print('Epoch [{}] -> Loss: {:.4f}  Acc: {:.4f} ({:.0f}m {:.0f}s)'.format(\n",
    "            epoch, sample_loss_avg, sample_acc_avg, time_elapsed//60, time_elapsed%60))\n",
    "        sample_acc_avg = 0\n",
    "        sample_loss_avg = 0\n",
    "\n",
    "    time_elapsed = time.time() - start_time\n",
    "    print()\n",
    "    print('Training completed in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    torch.save(encoder_model.state_dict(), './encoder_model.pth')\n",
    "    torch.save(decoder_model.state_dict(), './decoder_model.pth')\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total epochs: 43\n",
      "Epoch [1] -> Loss: 4.7283  Acc: 0.1713 (0m 30s)\n",
      "Epoch [2] -> Loss: 4.1803  Acc: 0.2323 (1m 3s)\n",
      "Epoch [3] -> Loss: 3.7425  Acc: 0.2725 (1m 38s)\n",
      "Epoch [4] -> Loss: 3.2971  Acc: 0.3202 (2m 12s)\n",
      "Epoch [5] -> Loss: 2.8733  Acc: 0.3685 (2m 48s)\n",
      "Epoch [6] -> Loss: 2.4677  Acc: 0.4331 (3m 21s)\n",
      "Epoch [7] -> Loss: 2.1092  Acc: 0.4884 (3m 53s)\n",
      "Epoch [8] -> Loss: 1.7705  Acc: 0.5524 (4m 26s)\n",
      "Epoch [9] -> Loss: 1.4821  Acc: 0.6011 (4m 58s)\n",
      "Epoch [10] -> Loss: 1.2208  Acc: 0.6654 (5m 31s)\n",
      "Epoch [11] -> Loss: 0.9910  Acc: 0.7196 (6m 3s)\n",
      "Epoch [12] -> Loss: 0.8116  Acc: 0.7730 (6m 37s)\n",
      "Epoch [13] -> Loss: 0.6401  Acc: 0.8267 (7m 12s)\n",
      "Epoch [14] -> Loss: 0.5347  Acc: 0.8502 (7m 48s)\n",
      "Epoch [15] -> Loss: 0.4393  Acc: 0.8747 (8m 27s)\n",
      "Epoch [16] -> Loss: 0.3778  Acc: 0.8916 (9m 2s)\n",
      "Epoch [17] -> Loss: 0.3321  Acc: 0.9016 (9m 38s)\n",
      "Epoch [18] -> Loss: 0.3033  Acc: 0.9069 (10m 12s)\n",
      "Epoch [19] -> Loss: 0.2718  Acc: 0.9138 (10m 47s)\n",
      "Epoch [20] -> Loss: 0.2620  Acc: 0.9155 (11m 22s)\n",
      "Epoch [21] -> Loss: 0.2590  Acc: 0.9131 (11m 57s)\n",
      "Epoch [22] -> Loss: 0.2416  Acc: 0.9172 (12m 33s)\n",
      "Epoch [23] -> Loss: 0.2312  Acc: 0.9195 (13m 8s)\n",
      "Epoch [24] -> Loss: 0.2259  Acc: 0.9206 (13m 41s)\n",
      "Epoch [25] -> Loss: 0.2205  Acc: 0.9194 (14m 15s)\n",
      "Epoch [26] -> Loss: 0.2196  Acc: 0.9213 (14m 50s)\n",
      "Epoch [27] -> Loss: 0.2134  Acc: 0.9212 (15m 26s)\n",
      "Epoch [28] -> Loss: 0.2150  Acc: 0.9203 (16m 1s)\n",
      "Epoch [29] -> Loss: 0.2070  Acc: 0.9217 (16m 37s)\n",
      "Epoch [30] -> Loss: 0.2041  Acc: 0.9232 (17m 13s)\n",
      "Epoch [31] -> Loss: 0.2041  Acc: 0.9214 (17m 48s)\n",
      "Epoch [32] -> Loss: 0.2019  Acc: 0.9212 (18m 22s)\n",
      "Epoch [33] -> Loss: 0.1990  Acc: 0.9215 (18m 55s)\n",
      "Epoch [34] -> Loss: 0.1933  Acc: 0.9224 (19m 30s)\n",
      "Epoch [35] -> Loss: 0.1838  Acc: 0.9256 (20m 4s)\n",
      "Epoch [36] -> Loss: 0.1908  Acc: 0.9248 (20m 41s)\n",
      "Epoch [37] -> Loss: 0.1876  Acc: 0.9239 (21m 16s)\n",
      "Epoch [38] -> Loss: 0.1872  Acc: 0.9234 (21m 49s)\n",
      "Epoch [39] -> Loss: 0.1941  Acc: 0.9208 (22m 23s)\n",
      "Epoch [40] -> Loss: 0.1904  Acc: 0.9241 (22m 58s)\n",
      "Epoch [41] -> Loss: 0.1770  Acc: 0.9272 (23m 31s)\n",
      "Epoch [42] -> Loss: 0.1882  Acc: 0.9248 (24m 6s)\n",
      "Epoch [43] -> Loss: 0.1841  Acc: 0.9247 (24m 41s)\n",
      "\n",
      "Training completed in 24m 41s\n"
     ]
    }
   ],
   "source": [
    "epochsc = int(100000/train_size)\n",
    "print('Total epochs:', epochsc)\n",
    "\n",
    "cost = train(encoder, decoder, encoder_optim, decoder_optim, criterion, epochs=epochsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb98ee6b650>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def translate(encoder_model, decoder_model, sentence, remove_spaces= False, toTensor=False):\n",
    "    with torch.no_grad():\n",
    "        sentence_translated = []\n",
    "        encoder_hidden = encoder_model.init_hidden().to(device)\n",
    "        \n",
    "        all_encoder_outputs = torch.zeros(MAX_LEN_SENTENCE, encoder.hidden_size, device=device)\n",
    "        for iw in range(sentence.size(0)):\n",
    "            encoder_output, encoder_hidden = encoder_model(sentence[iw], encoder_hidden) \n",
    "            #stores all encoder outputs to be used on the decoder\n",
    "            all_encoder_outputs[iw] = encoder_output[0, 0]\n",
    "        \n",
    "        decoder_hidden = encoder_hidden #the same hidden state is used for decoding\n",
    "        decoder_input = torch.tensor([[SOS]], dtype=torch.long, device=device)\n",
    "        for iw in range(MAX_LEN_SENTENCE):\n",
    "            decoder_output, decoder_hidden, attn_weights = decoder_model(decoder_input, decoder_hidden, all_encoder_outputs)\n",
    "            \n",
    "            _, guess = decoder_output.topk(1)\n",
    "            decoder_input = guess.squeeze().detach()\n",
    "            \n",
    "            if decoder_input.item() == EOS:\n",
    "                break\n",
    "            else:\n",
    "                sentence_translated.append(target_language.index2word[decoder_input.item()])\n",
    "                \n",
    "        sentence_translated = ' '.join(sentence_translated) \n",
    "        if remove_spaces:\n",
    "            sentence_translated = sentence_translated.replace(\" \", \"\")\n",
    "    return sentence_translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tensor2sentence(sentence, dictionary, remove_spaces=False):\n",
    "    s = []\n",
    "    for word in sentence:\n",
    "        s.append(dictionary.index2word[word.item()])\n",
    "        \n",
    "    del s[-1] # remove EOS\n",
    "    s = ' '.join(s)\n",
    "    if remove_spaces:\n",
    "        s = s.replace(\" \", \"\")\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation evaluation\n",
      "input  : they have already finished the work\n",
      "predict: 彼らはもうその仕事を終えた\n",
      "output : 彼らはもうその仕事を終えた\n",
      "-----------------\n",
      "input  : the storm didnt abate for several hours\n",
      "predict: 嵐は数時間おさまらなかった\n",
      "output : 嵐は数時間静まらなかった\n",
      "-----------------\n",
      "input  : thats right unfortunately\n",
      "predict: 残念ながらその通りです\n",
      "output : 残念ながらその通りです\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "print('Translation evaluation')\n",
    "for idx, (inp_sentence, targ_sentence) in enumerate(train_loader):\n",
    "    if idx >= 3:\n",
    "        break\n",
    "    \n",
    "    inp_sentence = inp_sentence[0].to(torch.long).to(device)\n",
    "    targ_sentence = targ_sentence[0].to(torch.long).to(device)\n",
    "    \n",
    "    print('input  :', tensor2sentence(inp_sentence, input_language))\n",
    "    print('predict:', translate(encoder, decoder, inp_sentence, remove_spaces=True))\n",
    "    print('output :', tensor2sentence(targ_sentence, target_language, remove_spaces=True))\n",
    "    print('-----------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
