{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from io import open\n",
    "import glob\n",
    "import unicodedata\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/*.txt'\n",
    "\n",
    "class LanguageDetect():\n",
    "    def __init__(self, files_path='./data', limit_size=None):\n",
    "        # scan language files\n",
    "        lang_files = glob.glob(data_path)\n",
    "        \n",
    "        self.alphabet = string.ascii_lowercase\n",
    "        self.wordlist = []\n",
    "        self.labels = []\n",
    "        self.classes = []\n",
    "        self.padding = 0\n",
    "\n",
    "        idx = 0\n",
    "        for file in lang_files:\n",
    "            language = os.path.splitext(os.path.basename(file))[0]\n",
    "            self.classes.append(language)\n",
    "            words = self._transform_vocab(file, limit_size)\n",
    "            self.wordlist += words\n",
    "            self.labels += [idx for i in range(len(words))]\n",
    "            idx += 1\n",
    "        \n",
    "        self.padding = len(max(self.wordlist, key=len)) # size of the largest string\n",
    "           \n",
    "    def _unicodeToAscii(self, s):\n",
    "        all_letters = string.ascii_letters\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "            and c in all_letters)\n",
    "\n",
    "    def _transform_vocab(self, vocab, limit_size):\n",
    "        wordset = (open(vocab).read()).lower() # open file and convert letters to lowercase\n",
    "        wordset = ''.join(i for i in wordset if (i.isalpha() | i.isspace())).split() # remove all non alpha and single characters thus split into a list\n",
    "        wordset = [word for word in wordset if len(word) > 1] # remove single characters and spaces\n",
    "        wordset = [self._unicodeToAscii(word) for word in wordset] # convert to ASCII\n",
    "        wordset = list(set(wordset)) # list of unique elements\n",
    "        if limit_size is not None:\n",
    "            wordset = wordset[:limit_size]\n",
    "        return wordset\n",
    "\n",
    "\n",
    "    def _word2tensor(self, word, padding=True):\n",
    "        if padding:\n",
    "            num_charac = self.padding # pad to the fill the size\n",
    "        else:\n",
    "            num_charac = len(word)\n",
    "        len_word = len(word)\n",
    "        wtensor = torch.zeros(num_charac, 1, len(self.alphabet))\n",
    "        for iletter in range(len_word):\n",
    "            wtensor[iletter][0][self.alphabet.find(word[iletter])] = 1\n",
    "        return wtensor\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        word = self.wordlist[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # convert a word to a tensor (len word x 1 x alphabet)\n",
    "        word = self._word2tensor(word, padding=True)\n",
    "        \n",
    "        return word, label\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LanguageDetect(data_path, limit_size=200)\n",
    "alphabet = dataset.alphabet\n",
    "\n",
    "classes = dataset.classes\n",
    "print('dataset size:', len(dataset))\n",
    "print('classes:', classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor2word(tensor):\n",
    "    alphabet = string.ascii_lowercase\n",
    "    word = ''\n",
    "    for i in range(tensor.size()[0]):\n",
    "        for j in range(tensor.size()[2]):\n",
    "            if tensor[i][0][j] == 1:\n",
    "                word += alphabet[j]\n",
    "    return word\n",
    "        \n",
    "    \n",
    "# dataset dim = |idx (input,label) \n",
    "# first input\n",
    "print(dataset[0][0].size())\n",
    "# first label\n",
    "print(dataset[0][1])\n",
    "\n",
    "\n",
    "# sample example\n",
    "sample_idx = np.random.randint(1, len(dataset))\n",
    "\n",
    "print('random sample')\n",
    "print('language:', classes[dataset[sample_idx][1]])\n",
    "print('word:', tensor2word(dataset[sample_idx][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_data, val_data = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# creating dataloaders\n",
    "batch_size = 1\n",
    "\n",
    "# create training data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# create validation data loader\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN model](./model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple RNN as in: \n",
    "# https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.i2h = nn.Linear(input_size+hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size+hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, last_hidden):\n",
    "        combined = torch.cat((input, last_hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return hidden, output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Using gpu:\", use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(len(alphabet), hidden_size, len(classes)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_output(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return classes[category_i], category_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def train(model, criterion, learning_rate=0.0001, epochs=1000, plots_per_epoch=100):\n",
    "    \n",
    "    cost = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            hidden = torch.zeros(1, hidden_size).to(device)\n",
    "            rnn.zero_grad()\n",
    "\n",
    "            word = inputs[0]\n",
    "            for i in range(word.size()[0]): # go through each letter\n",
    "                if torch.sum(word[i]) > 0: # avoid paddding\n",
    "                    hidden, output = rnn(word[i], hidden)\n",
    "                else: break\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # Add parameters' gradients to their values, multiplied by learning rate\n",
    "            for p in rnn.parameters():\n",
    "                p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "            #current_loss += loss\n",
    "            guess, guess_i = classify_output(output)\n",
    "            if guess_i == labels:\n",
    "                running_corrects += 1\n",
    "            running_loss += loss\n",
    "            \n",
    "        cost.append(running_loss)\n",
    "        epoch_acc = running_corrects / train_size\n",
    "        \n",
    "        if epoch % plots_per_epoch == 0:\n",
    "            print('Epoch [{}] -> Loss: {:.4f}  Acc: {:.4f}'.format(\n",
    "                epoch, running_loss/train_size, epoch_acc))\n",
    "\n",
    "    \n",
    "    time_elapsed = time.time() - start_time\n",
    "    print()\n",
    "    print('Training completed in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "    return model, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, all_losses = train(rnn, criterion, learning_rate, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "\n",
    "            \n",
    "def calc_accuracy(model, dataloader, print_output=False):\n",
    "    num_correct = 0\n",
    "    num_examples = len(dataloader.dataset)                       # test DATA not test LOADER\n",
    "    for inputs, labels in dataloader:                  # for all exampls, over all mini-batches in the test dataset\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        predictions = evaluate(inputs[0])\n",
    "        \n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        if print_output:\n",
    "            correct = '✓' if guess_i == labels else '✗ (%s)' % classes[labels]\n",
    "            print('%s / %s %s' % (tensor2word(inputs[0]), guess, correct))\n",
    "        \n",
    "        if(guess_i == labels):\n",
    "            num_correct += 1\n",
    "               \n",
    "    percent_correct = num_correct / num_examples * 100\n",
    "    return percent_correct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
